export type KeywordCard = {
  title: string;
  body: string;
};

export type Keyword = {
  id: string;
  label: string;
  summary: string;
  cards: KeywordCard[];
};

export const keywords: Keyword[] = [
  {
    id: "llm",
    label: "LLM",
    summary:
      "대규모 언어 모델은 사전학습과 지시 튜닝을 거쳐 코드·문서·이미지를 아우르는 범용 두뇌로 진화하고 있습니다. 품질과 토큰 단가는 모델마다 크게 달라 워크로드에 따라 포트폴리오를 나눠 운용해야 합니다. 멀티모달 입력과 툴 호출 기능이 빠르게 붙고 있어, 모델 그 자체보다 이를 둘러싼 파이프라인 설계 역량이 중요해졌습니다. 실무에서는 모델 선택표·비용 가이드·평가 루프를 세트로 묶어 관리해야 변화에 흔들리지 않습니다.",
    cards: [
      {
        title: "모델 포트폴리오",
        body: "고품질(Claude 3.5, GPT-4o), 균형형(GPT-4.1 미니, Sonnet)과 경량(3.5 계열, Haiku)의 특성이 뚜렷합니다. 팀에서 반복 사용하는 시나리오를 정리해 어떤 모델이 어떤 목적에 쓰이는지 명시적으로 관리해야 비용과 성능이 예측됩니다. 분기마다 토큰 집계 리포트를 공유하면 모델 변경 논의를 훨씬 수월하게 가져갈 수 있습니다.",
      },
      {
        title: "컨텍스트 전략",
        body: "긴 컨텍스트는 편리하지만 비용이 급증하므로 핵심 스팬 추출, 요약, chunk 재조합으로 토큰을 절약합니다. 질문 유형별 프롬프트 템플릿을 분리하면 동일 모델에서도 응답 품질 편차를 줄일 수 있습니다. ‘어떤 정보가 언제 필요한가’를 로그로 남겨두면 이후 RAG/툴콜 설계와도 자연스럽게 연결됩니다.",
      },
      {
        title: "안전/가드레일",
        body: "시스템 프롬프트, 입력 정제, 출력 검증 계층이 기본값이며, 필요 시 별도 안전 모델로 2차 필터링을 붙입니다. 로그를 남겨 정책 위반 시점과 재현 경로를 추적할 수 있어야 운영이 가능합니다. 가드레일 규칙을 정책 문서와 Git으로 함께 관리하면 컴플라이언스 대응도 빨라집니다.",
      },
      {
        title: "멀티모달",
        body: "텍스트·이미지·음성 입력을 동시에 받아 코드를 설명하거나 UI 스크린샷을 분석하는 시나리오가 늘고 있습니다. 입력 채널마다 토큰 계산 방식이 달라 비용 산정표를 미리 만들어 두면 도입 속도가 빨라집니다. 실습 기록과 샘플 프롬프트를 저장소에 모아두면 온보딩 시나리오에도 재활용이 가능합니다.",
      },
    ],
  },
  {
    id: "rag",
    label: "RAG",
    summary:
      "Retrieval-Augmented Generation은 사내 문서나 코드를 조회해 모델에 필요한 근거를 전달함으로써 정확도와 책임감을 동시에 확보합니다. 인덱싱·검색·컨텍스트 구성·검증까지 이어지는 전 단계의 품질이 응답 품질을 좌우합니다. 운영 단계에서는 데이터 최신성과 접근 권한, 피드백 루프를 함께 설계해야 오래 유지됩니다. FAQ·런북·정책 같은 핵심 자산을 우선순위로 묶어 순차적으로 확장하면 투자 대비 효과가 분명해집니다.",
    cards: [
      {
        title: "인덱싱 설계",
        body: "Chunk 길이, 중첩 범위, 메타데이터 태그(저자, 버전, 팀)를 어떻게 붙이느냐가 검색 품질을 결정합니다. 초기 구축 시점에 섹션별 커스텀 파서를 만들어 두면 이후 유지보수 비용을 크게 줄일 수 있습니다. 장기적으로는 변경 이력을 자동 감지해 재인덱싱하는 파이프라인을 붙여야 품질이 유지됩니다.",
      },
      {
        title: "순위/리랭크",
        body: "BM25와 벡터 검색을 하이브리드로 돌리고 Cross-encoder 혹은 LLM 리랭커로 정밀도를 높입니다. 쿼리 유형별로 스코어 가중치를 달리 두면 불필요한 토큰 낭비 없이 원하는 문서를 노출시킬 수 있습니다. 검색 로그를 분석해 ‘못 찾은 질문’을 분류하면 향후 데이터 보강 로드맵을 세울 수 있습니다.",
      },
      {
        title: "컨텍스트 압축",
        body: "요약, 계층적 chunking, 슬라이딩 윈도 기법으로 토큰을 줄이면서 근거 스팬을 유지합니다. 긴 맥락을 단순히 잘라 붙이는 대신 역할에 따라 섹션을 구조화하면 모델이 정보 우선순위를 이해하기 쉽습니다. 압축 결과를 사람이 검증해 피드백을 남기는 루틴이 있어야 자동화 전략이 안정적으로 굴러갑니다.",
      },
      {
        title: "검증/출처",
        body: "출처 링크와 특정 문장을 함께 노출하고, 체이닝 시 단계별 검증 질문을 넣어 환각을 줄입니다. 사용자가 ‘맞다/아니다’를 클릭할 수 있는 피드백 UI를 붙이면 지속적인 품질 개선에 도움이 됩니다. 이 피드백을 주기적으로 분석해 어떤 데이터셋이 거짓 양성/음성을 만들었는지 역추적하세요.",
      },
    ],
  },
  {
    id: "mcp",
    label: "MCP",
    summary:
      "Model Context Protocol은 모델이 API·툴·데이터 자원을 표준화된 방식으로 호출하도록 돕는 개방형 사양입니다. 모델과 툴 제공자를 분리해 교체 가능성을 확보하고, 다중 모델·멀티모달 환경에서도 동일한 계약으로 확장할 수 있게 해 줍니다. 플랫폼에 MCP를 붙이면 조직 내 다양한 팀이 공통된 인터페이스로 AI 기능을 노출할 수 있습니다. 사내 시스템을 순차적으로 연결할 수 있어 ‘작은 성공’을 빠르게 반복하기 좋은 접근입니다.",
    cards: [
      {
        title: "표준 API 표현",
        body: "툴, 리소스, 텍스트 스트림을 JSON 기반 스키마로 묶어 모델이 이해할 수 있는 형태로 노출합니다. 규격이 정해져 있어 새 툴을 추가할 때마다 재학습 없이도 바로 붙일 수 있습니다. 사내 표준 스키마 저장소를 두고 리뷰 프로세스를 붙이면 품질이 일정해집니다.",
      },
      {
        title: "모델-툴 분리",
        body: "에이전트/모델과 툴 제공자를 분리하면 모델 교체나 버전 업이 쉬워집니다. 동일한 툴 레이어를 여러 모델이 공유하므로 거버넌스도 단일화됩니다. 실제 운영에서는 ‘필수/옵션 툴’ 리스트를 문서화해 모델 업데이트 시점에 체크하도록 합니다.",
      },
      {
        title: "보안/권한",
        body: "툴 정의 시 파라미터, 입력 검증 규칙, 실행 권한을 명시해 두면 사고를 예방할 수 있습니다. 실행 이력과 감사 로그를 MCP 단에서 수집할 수 있는 것도 장점입니다. 권한 정책을 IaC처럼 버전 관리하면 보안/컴플라이언스 팀과의 협업도 쉬워집니다.",
      },
      {
        title: "멀티모달 준비",
        body: "텍스트뿐 아니라 이미지, 오디오, 파일 리소스를 동일한 패턴으로 다룰 수 있도록 확장 포인트가 열려 있습니다. 팀 단위 도구가 늘어날수록 이 일관성이 진가를 발휘합니다. 실험적인 채널부터 MCP에 연결해 학습한 후 점차 미션 크리티컬한 시스템으로 확대하세요.",
      },
    ],
  },
  {
    id: "coding-assist",
    label: "Coding Assistance",
    summary:
      "IDE와 리포지토리에 붙는 코드 보조 도구는 반복적인 구현과 테스트, 리뷰 작업을 단축해 개발자를 핵심 판단에 집중하게 합니다. 코드 맥락을 얼마나 정확하게 주입했는지에 따라 품질 편차가 크기 때문에, 도구 설정이 곧 도입 효과입니다. 팀 규칙과 체크리스트를 프롬프트나 룰로 연결해 두면 일관성이 유지됩니다. 도입 후에는 ‘AI 추천을 그대로 사용한 비율’을 측정해 실제 생산성 향상 여부를 추적하세요.",
    cards: [
      {
        title: "컨텍스트 주입",
        body: "현재 편집 중인 파일 뿐 아니라 관련 테스트, 스키마, 다이어그램을 함께 던져야 제안 품질이 올라갑니다. 프로젝트별 컨텍스트 세트를 미리 정의해두면 새 구성원이 바로 동일 환경을 쓸 수 있습니다. Git 훅이나 VSCode 확장으로 컨텍스트 번들을 자동 갱신하면 누락을 예방할 수 있습니다.",
      },
      {
        title: "체크리스트 기반",
        body: "리뷰 규칙이나 리팩터링 절차를 명시적 체크리스트로 만들어 프롬프트에 연결합니다. 결과가 체계적으로 나오기 때문에 사람이 검증하기도 쉬워집니다. 체크리스트 항목을 주기적으로 리팩터링 로그와 비교해 갱신하면 실무 위상도 유지됩니다.",
      },
      {
        title: "Diff 우선",
        body: "전체 파일 대신 변경 diff를 먼저 전달하면 비용이 줄고 집중도가 높아집니다. 영향 분석이 필요할 때만 주변 파일을 추가로 주입하는 방식이 효율적입니다. Diff 요약과 위험 구역을 함께 제공하면 리뷰어가 바로 후속 질문을 만들 수 있습니다.",
      },
      {
        title: "IDE 액션",
        body: "“이 함수만 리팩터링”, “테스트 3개 생성”처럼 구체적 액션 단위로 요청하면 정확도가 높습니다. 액션별 템플릿을 스니펫으로 저장해두면 실무에서 재사용성이 좋아집니다. 명령어-결과를 캡처해 플레이북으로 남기면 팀 온보딩 자료가 자연스럽게 채워집니다.",
      },
    ],
  },
  {
    id: "prompting",
    label: "Prompt Engineering",
    summary:
      "프롬프트 엔지니어링은 모델의 역할, 목적, 금지 규칙을 언어로 설계해 일관된 출력을 끌어내는 작업입니다. 체이닝과 도구 호출이 늘어날수록 각 단계의 프롬프트 규격을 명확히 해 두어야 디버깅이 쉬워집니다. 실험과 평가 데이터를 남기면 팀 단위로 지식을 축적할 수 있습니다. 결과와 로그를 버전 관리하면 모델/프롬프트가 자주 바뀌더라도 품질을 수치로 비교할 수 있습니다.",
    cards: [
      {
        title: "역할/목표 고정",
        body: "시스템 프롬프트에서 역할, 성공 조건, 금지사항을 구체적으로 명시합니다. 이 정보를 별도 파일로 관리하면 버전 차이를 추적하기 좋습니다. 실무에서는 역할 정의에 Metric/KPI를 함께 적어 두면 리뷰 시점에도 기준이 명확해집니다.",
      },
      {
        title: "출력 스키마",
        body: "JSON/마크다운 스키마와 검증 루프를 붙여 후처리 오류를 줄입니다. Schema + Validator 조합을 재사용하면 새로운 기능도 빠르게 붙일 수 있습니다. 출력 포맷 예제를 최소 2~3개 보관하면 회귀 테스트 작성도 쉬워집니다.",
      },
      {
        title: "예제/컨텍스트",
        body: "좋은/나쁜 예시를 함께 제공하면 분기 처리 정확도가 올라갑니다. 예시 세트는 주기적으로 갱신해 최신 버전의 모델과도 맞춰줘야 합니다. 예시 데이터에 태그를 붙여 어떤 규칙을 강조하는지 표시하면 협업 시 설명이 수월합니다.",
      },
      {
        title: "평가 루프",
        body: "자동/반자동 평가를 붙여 프롬프트 변경 효과를 수치로 추적합니다. 실험 기록을 남겨 다른 팀원도 동일한 조건에서 재현할 수 있게 합니다. 평가 리포트에 ‘다음 액션’을 명시하면 프롬프트가 방치되지 않고 계속 진화합니다.",
      },
    ],
  },
  {
    id: "context-eng",
    label: "Context Engineering",
    summary:
      "컨텍스트 엔지니어링은 모델에 어떤 정보를 어떤 길이로 넣을지 설계하는 작업으로, RAG·프롬프트·툴콜 전반의 품질을 좌우합니다. 정보의 우선순위를 정하고, 공통 맥락을 캐시하며, 필요 시 압축하는 전략이 필요합니다. 비용과 정확도의 균형을 찾는 반복적인 실험이 핵심입니다. 대화 로그와 Eval 결과를 묶어 ‘컨텍스트 다이어트’를 주기적으로 실행하면 토큰 낭비를 통제할 수 있습니다.",
    cards: [
      {
        title: "스팬 선택",
        body: "사용자 질문과 직접적인 연관이 있는 스팬만 남기고 나머지는 요약하거나 제거합니다. 질문 분류기를 붙여 적절한 컨텍스트를 자동 매핑하면 효율이 올라갑니다. 스팬 선택 기준을 표 형태로 남겨두면 도메인이 달라져도 빠르게 재사용할 수 있습니다.",
      },
      {
        title: "계층 캐시",
        body: "프롬프트 공통 구간을 캐싱하거나 재사용해 토큰을 절약합니다. 주기적으로 캐시를 검증해 구버전 데이터가 남지 않도록 해야 합니다. 캐시 miss/ hit 비율을 대시보드화하면 어느 영역을 더 최적화해야 할지 보입니다.",
      },
      {
        title: "구조화 포맷",
        body: "목록, 테이블, JSON과 같은 구조화 포맷으로 변환하면 모델이 파싱에 쓰는 토큰을 줄입니다. 특히 계산형 질문에는 테이블 포맷이 효과적입니다. 출력-입력 예시를 함께 저장하면 자동 변환 스크립트를 테스트하기 쉬워집니다.",
      },
      {
        title: "품질-비용 트레이드오프",
        body: "긴 맥락이 항상 정답을 보장하지 않으므로 Eval로 적정 길이를 탐색합니다. 토큰 한도를 실시간으로 모니터링해 경고를 띄우면 사고를 예방할 수 있습니다. KPI에 ‘토큰당 해결률’을 추가해두면 의사결정 시 확실한 근거가 됩니다.",
      },
    ],
  },
  {
    id: "tool-calling",
    label: "Tool / Function Calling",
    summary:
      "툴/함수 호출은 모델이 외부 API, 데이터베이스, 코드 실행 환경과 상호작용하며 실제 액션을 수행하게 해 줍니다. 명세를 어떻게 설계했는지, 호출 결과를 어떻게 검증했는지가 곧 신뢰도입니다. 실패 시 재시도 전략과 감사 로그를 기본값으로 두어야 운영이 가능합니다. 작은 자동화 시나리오부터 배포해 경험을 쌓고, 성공 케이스를 기반으로 제어 범위를 넓히는 것이 안전합니다.",
    cards: [
      {
        title: "함수 스키마",
        body: "필수·옵셔널 파라미터, enum, 범위 제한을 명확히 하여 안전한 호출을 강제합니다. 스키마를 문서화하면 모델뿐 아니라 사람도 동일한 계약을 공유하게 됩니다. 샘플 호출을 함께 보관하면 새 모델 혹은 외부 협력사와의 연동 테스트도 빠릅니다.",
      },
      {
        title: "후처리 검증",
        body: "결과를 규칙 기반 혹은 추가 모델로 검증하고, 실패 시 재시도 루틴을 제공합니다. 호출 실패 데이터를 수집하면 이후 프롬프트를 튜닝할 근거가 됩니다. 실패 유형을 태그화하면 어디에서 사람이 개입해야 하는지 명확해집니다.",
      },
      {
        title: "Idempotent",
        body: "동일 호출이 중복 실행될 수 있으므로 락이나 트랜잭션으로 보호합니다. 감사 로그를 남겨 어떤 입력이 어떤 시스템 액션을 유발했는지 추적할 수 있게 합니다. 재시도 정책과 함께 문서화하면 장애 상황에서도 바로 대응할 수 있습니다.",
      },
      {
        title: "권한 경계",
        body: "모델이 접근 가능한 기능 범위를 최소화하고 세분화된 모니터링을 붙입니다. 환경변수, API 키를 툴 레이어에서 안전하게 주입해야 보안 사고를 줄일 수 있습니다. 권한 요청/승인 워크플로를 명확히 해 두면 감사 대응도 수월합니다.",
      },
    ],
  },
  {
    id: "eval-llmops",
    label: "Eval / LLMOps",
    summary:
      "Eval/LLMOps는 프롬프트와 파이프라인 품질을 측정하고 배포 전후 변화를 추적하는 운영 문화입니다. 자동+휴먼 평가, 실험 기록, 로그 관측이 삼박자를 이뤄야 회귀를 빠르게 잡아낼 수 있습니다. 모델이나 프롬프트가 자주 바뀌는 만큼 실험 데이터 자산을 쌓는 것이 곧 경쟁력입니다. KPI, 경보 조건, 운영 매뉴얼을 한 곳에 모아두면 조직 차원의 신뢰도를 확보할 수 있습니다.",
    cards: [
      {
        title: "메트릭 설계",
        body: "정확도, 근거성, 포맷 준수, 독성/안전 등 과제별 지표를 구체적으로 정의합니다. 팀 합의된 메트릭이 있어야 실험 결과를 비교할 수 있습니다. 지표 정의와 계산 예시를 위키에 남겨두면 새 실험도 동일한 기준으로 측정됩니다.",
      },
      {
        title: "샘플 세트",
        body: "회귀 세트와 신규 기능 세트를 분리해 관리하고, 난이도나 사용 빈도에 따라 가중치를 둡니다. 샘플에 태그를 붙여 어떤 케이스가 실패했는지 바로 추적합니다. 세트를 Git LFS나 데이터 카탈로그에 보관해 버전 간 차이를 명확히 남겨두세요.",
      },
      {
        title: "가드레일 테스트",
        body: "금지된 행동·출력 케이스를 반복적으로 돌려 정책 위반을 조기에 잡습니다. UI/UX와 연결해 팀원 누구나 테스트를 재실행할 수 있게 만드는 것이 좋습니다. 테스트 결과를 CI 파이프라인에 통합하면 배포 전 경고 체계도 자동화됩니다.",
      },
      {
        title: "관측/로그",
        body: "프롬프트/모델 버전, 토큰 사용량, 오류·재시도 로그를 수집해 대시보드화합니다. 관측 데이터를 기초로 SLA를 정의하면 비즈니스와의 대화가 수월해집니다. 운영 이벤트와 메모를 함께 기록하면 문제 재현과 회고가 훨씬 빨라집니다.",
      },
    ],
  },
];

